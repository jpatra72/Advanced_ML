{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Playground.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP24JfYbVHTU2yYBW/DZR9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpatra72/Advanced_ML/blob/main/LSTM_Playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Source Links to Playground Activity\n",
        "*   *Outputs of LSTM: https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm*\n",
        "\n",
        "*   *How to update LSTM during training: https://machinelearningmastery.com/update-lstm-networks-training-time-series-forecasting*\n",
        "\n",
        "*   *How to build LSTM (tf): https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767#.ozeai0fo8*\n",
        "\n",
        "* *BPTT in LSTM: https://stats.stackexchange.com/questions/219914/rnns-when-to-apply-bptt-and-or-update-weights*\n",
        "\n",
        "* *https://machinelearningmastery.com/prepare-univariate-time-series-data-long-short-term-memory-networks/*\n",
        "\n",
        "* *Keras LSTM diagram to understand Batch: https://github.com/MohammadFneish7/Keras_LSTM_Diagram*\n",
        "\n",
        "* *Stateless vs Statefull and Subsequencing: http://philipperemy.github.io/keras-stateful-lstm/*\n",
        "\n",
        "* *Pytorch vs TensorFlow - Stateless vs Statefull: https://discuss.pytorch.org/t/confusion-regarding-pytorch-lstms-compared-to-keras-stateful-lstm/44502/5*\n",
        "\n",
        "* *Stateful w/ Subsequencing:  https://gist.github.com/spacegoing/7935e5c2f0c8fa2f0719d2e729e794e8#file-test_stateful_lstm-py-L22*\n",
        "\n",
        "* *Pytorch forward Implementation: https://towardsdatascience.com/whats-happening-in-my-lstm-layer-dd8110ecc52f*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "N5qnc3C-uxe5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIUw_zQYW-Mc"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "\n",
        "torch.manual_seed(1)\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)] # indicates that there are 5 sequences to be given as inputs and (1,3) indicates that there is 1 layer with 3 cells\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3)) # initializing h and c values to be of dimensions (1, 1, 3) which \n",
        "                                #indicates there is (1 * 1) - num_layers * num_directions, with batch size of 1 and projection size of 3. \n",
        "                                # Since there is only 1 batch in input, h and c can also have only one batch of data for initialization \n",
        "                                #and the number of cells in both input and output should also match.\n",
        " \n",
        "lstm = nn.LSTM(3, 3) #implying both input and output are 3 dimensional data\n",
        "# summary(lstm, input_size=(1,3))\n",
        "for i in inputs:\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "    print('out:', out)\n",
        "    print('hidden:', hidden, '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 2\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)] \n",
        "hidden = (torch.randn(2, 1, 3),\n",
        "          torch.randn(2, 1, 3))\n",
        "lstm = nn.LSTM(input_size=3, hidden_size=3, num_layers=2)\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "    print('out:', out)\n",
        "    print('hidden:', hidden, '\\n')"
      ],
      "metadata": {
        "id": "ZwwoSjG7pihY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "lstm = nn.LSTM( input_size = 1, hidden_size = 20, num_layers  = 1 )\n",
        "x = torch.rand( 50, 1, 1)\n",
        "output, (hn, cn) = lstm(x)\n",
        "output.size()"
      ],
      "metadata": {
        "id": "MzwgQk5xuCdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
        "# print(inputs)\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3))\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "\n",
        "# alternatively, we can do the entire sequence all at once.\n",
        "# the first value returned by LSTM is all of the hidden states throughout\n",
        "# the sequence. the second is just the most recent hidden state\n",
        "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
        "# The reason for this is that:\n",
        "# \"out\" will give you access to all hidden states in the sequence\n",
        "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
        "# by passing it as an argument  to the lstm at a later time\n",
        "# Add the extra 2nd dimension\n",
        "# print(len(inputs))\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "# print((inputs), '\\n')\n",
        "\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "print((hidden))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIR2NdGPMalE",
        "outputId": "18bfc639-f74a-42df-a147-e32852851148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.3653,  0.0123, -0.4226]],\n",
            "\n",
            "        [[ 0.1415,  0.1479, -0.2528]],\n",
            "\n",
            "        [[ 0.4234,  0.0467, -0.1540]],\n",
            "\n",
            "        [[ 0.5676, -0.1238,  0.0710]],\n",
            "\n",
            "        [[ 0.7421, -0.0026,  0.2334]]], grad_fn=<StackBackward0>)\n",
            "(tensor([[[ 0.7421, -0.0026,  0.2334]]], grad_fn=<StackBackward0>), tensor([[[ 1.3536, -0.0061,  0.3241]]], grad_fn=<StackBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "import numpy as np\n",
        "from numpy.random import choice\n",
        "\n",
        "\n",
        "def prepare_sequences(x_train, window_length):\n",
        "  windows = []\n",
        "  for i, sequence in enumerate(x_train):\n",
        "    for window_start in range(0, T - window_length + 1):\n",
        "      window_end = window_start + window_length\n",
        "      window = sequence[window_start:window_end]\n",
        "      windows.append(window)\n",
        "  return np.array(windows)\n",
        "\n",
        "\n",
        "def get_sequential_batch(bX_train, bY_train, N_train, batch_size):\n",
        "  bX_train = bX_train.reshape(N_train, T - window_length + 1, window_length)\n",
        "  N = N_train - N_train % batch_size\n",
        "  for i in range(0, N, batch_size):\n",
        "    for t in range(T - window_length + 1):\n",
        "      bX = bX_train[i:i + batch_size, t, :]\n",
        "      bY = bY_train[i:i + batch_size]\n",
        "      yield bX[..., np.newaxis], bY[..., np.newaxis], t\n",
        "      # yield bX, bY, t\n",
        "\n",
        "\n",
        "## hyper parameters\n",
        "debug = True\n",
        "N = 1200\n",
        "T = 20\n",
        "N_train = 1000\n",
        "N_test = N - N_train\n",
        "window_length = 10\n",
        "batch_size = 32\n",
        "epochs = 4\n",
        "# if stateful = True, test acc = 1.0; False, test acc = 0.5\n",
        "stateful = False\n",
        "\n",
        "## create train / test dataset\n",
        "data = np.zeros([N, T])\n",
        "one_indexes = choice(a=N, size=N // 2, replace=False)\n",
        "data[one_indexes, 0] = 1  # very long term memory.\n",
        "X_train = data[:N_train]\n",
        "Y_train = X_train[:, 0]\n",
        "X_test = data[N_train:]\n",
        "Y_test = X_test[:, 0]\n",
        "\n",
        "## create model\n",
        "model = Sequential()\n",
        "model.add(\n",
        "    LSTM(\n",
        "        3,\n",
        "        batch_input_shape=(batch_size, window_length, 1),\n",
        "        return_sequences=False,\n",
        "        stateful=stateful))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(\n",
        "    loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "## training loop\n",
        "for e in range(epochs):\n",
        "  # train data generator\n",
        "  bX_train = prepare_sequences(X_train, window_length)\n",
        "  # print(bX_train.reshape(N_train, T - window_length + 1, window_length).shape)\n",
        "  x_train_batch_gen = get_sequential_batch(bX_train, Y_train, N_train,\n",
        "                                           batch_size)\n",
        "  for bX, bY, t in x_train_batch_gen:\n",
        "    print(bX.shape, t)\n",
        "    loss, acc = model.train_on_batch(bX, bY)\n",
        "    tr_loss.append(loss)\n",
        "    tr_acc.append(acc)\n",
        "    counter += 1\n",
        "\n",
        "    if counter == 1 and debug:\n",
        "      t_dataset.append(\n",
        "          sum(bY[:, 0] == bX[:, 0, :].reshape(-1)) + int(bX.sum() == bY.sum()))\n",
        "\n",
        "    # reset states\n",
        "    if counter == T - window_length + 1:\n",
        "      model.reset_states()\n",
        "      counter = 0\n",
        "  print(np.mean(tr_acc))\n",
        "  # debug\n",
        "  if debug:\n",
        "    print(np.mean(t_dataset))\n",
        "\n"
      ],
      "metadata": {
        "id": "DGXJsrfN0PEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class SimpleLSTM(nn.Module):\n",
        "  \"\"\"implements a 'simple' lstm - a single/multilayer uni/bi directional lstm with a single output\"\"\"\n",
        "  def __init__(self, n_features, window_size, \n",
        "               output_size, h_size, n_layers=1, \n",
        "               bidirectional=False, device=torch.device('cpu')):\n",
        "    super().__init__()\n",
        "    self.n_features = n_features\n",
        "    self.window_size = window_size\n",
        "    self.output_size = output_size\n",
        "    self.h_size = h_size\n",
        "    self.n_layers = n_layers\n",
        "    self.directions = 2 if bidirectional else 1\n",
        "    self.device = device\n",
        "\n",
        "    # our layer of interest\n",
        "    self.lstm = nn.LSTM(input_size=n_features, hidden_size=h_size, \n",
        "                        num_layers=n_layers, bidirectional=bidirectional, batch_first=True)\n",
        "    self.hidden = None\n",
        "    \n",
        "    self.linear = nn.Linear(self.h_size * self.directions, self.output_size)\n",
        "    \n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    \n",
        "    hidden_state  = torch.randn(self.n_layers * self.directions,\n",
        "                            batch_size ,self.h_size).to(self.device)\n",
        "    cell_state  = torch.randn(self.n_layers * self.directions, \n",
        "                           batch_size,self.h_size).to(self.device)\n",
        "    \n",
        "    hidden_state = Variable(hidden_state)\n",
        "    cell_state = Variable(cell_state)\n",
        "\n",
        "    return (hidden_state, cell_state) \n",
        "\n",
        "  def forward(self, input):\n",
        "    batch_size = list(input.size())[0]\n",
        "    self.hidden = self.init_hidden(batch_size)\n",
        "    lstm_output, self.hidden = self.lstm(input, self.hidden)\n",
        "    print(\"lstm_output:\", lstm_output.shape)\n",
        "    print(\"hidden:\", len(self.hidden), self.hidden[0].shape)\n",
        "    last_hidden_states = torch.index_select(lstm_output, 1,  index=torch.LongTensor(([self.window_size-1])))\n",
        "    predictions = self.linear(last_hidden_states)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "model = SimpleLSTM(n_features=23, window_size=6, output_size=1, h_size=256)\n",
        "\n",
        "data = torch.rand((100,6, 23))\n",
        "\n",
        "print(model.forward(data).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FvwU8S80U-C",
        "outputId": "0f358989-505b-47df-d4c6-1d1954c3aa7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object get_sequential_batch at 0x7f906977bed0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# Custom Dataset\n",
        "class TensorDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, TensorX,TensorY):\n",
        "        self.TensorX = TensorX\n",
        "        self.TensorY = TensorY\n",
        "    def __len__(self):\n",
        "        return self.TensorX.shape[0]\n",
        "    def __getitem__(self,idx):\n",
        "        return (self.TensorX[idx],self.TensorY[idx])\n",
        "\n",
        "# Model = Stateful LSTM+linear\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size,hidden_size,output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.lstm = torch.nn.LSTM(batch_first=True,input_size=input_size,hidden_size=hidden_size)\n",
        "        self.linear = torch.nn.Linear(in_features=hidden_size, out_features=output_size)\n",
        "    def forward(self, x, hn, cn):\n",
        "        # Stateful\n",
        "        x_longer = x.view(1,x.shape[0]*x.shape[1],x.shape[2])\n",
        "        out_longer, (hn, cn) = self.lstm(x_longer, (hn.detach(), cn.detach()))\n",
        "        out = out_longer.view(x.shape[0],x.shape[1],out_longer.shape[2])\n",
        "        print(\"output pre linear layer: \", out, out.shape)\n",
        "        print(out[:,-1,:], out[:,-1,:].shape)\n",
        "        out = self.linear(out[:,-1,:])\n",
        "        print(\"output post linear layer: \", out, out.shape)\n",
        "        return out.unsqueeze(-1), (hn, cn)\n",
        "\n",
        "N_epochs = 1\n",
        "hidden_size = 2\n",
        "features = 1\n",
        "learning_rate = 0.001\n",
        "batch_size=2\n",
        "output_size = 1\n",
        "model = LSTM(input_size=features,hidden_size=hidden_size,output_size=output_size)#Create model\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)#optimizer\n",
        "criterion = torch.nn.MSELoss() # loss\n",
        "# Create dataset: Imagine original_batch_size=2\n",
        "x = torch.tensor([[1.0, 2.0, 3.0],[4.0, 5.0, 6.0],[7.0, 8.0, 9.0],[10.0, 11.0, 12.0]]).unsqueeze(-1)\n",
        "y = torch.tensor([[4.],[7.],[10.],[13.]]).unsqueeze(-1)\n",
        "dataset = TensorDataset(x,y)\n",
        "dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size)\n",
        "print(\"head from dataloader: \", next(iter(dataloader))[1].shape)\n",
        "# Training\n",
        "for epoch in range(0,N_epochs):\n",
        "    # Create first hidden and cell state with batch=1 \n",
        "    hn = torch.zeros(1, 1, hidden_size)#[num_layers*num_directions,batch,hidden_size]\n",
        "    cn = torch.zeros(1, 1, hidden_size)#[num_layers*num_directions,batch,hidden_size]\n",
        "    for x,y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        out, (hn,cn) = model(x,hn,cn)\n",
        "        loss = criterion(out,y)\n",
        "        loss.backward()# Backward\n",
        "        optimizer.step()# gradient descent on adam step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q0utQkXCyZ4",
        "outputId": "57ae3f6b-cb97-46ac-b0bd-91574f975347"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "head from dataloader:  torch.Size([2, 1, 1])\n",
            "output pre linear layer:  tensor([[[-0.1933, -0.0227],\n",
            "         [-0.4396,  0.1259],\n",
            "         [-0.5998,  0.4198]],\n",
            "\n",
            "        [[-0.6966,  0.6801],\n",
            "         [-0.7582,  0.7975],\n",
            "         [-0.7991,  0.8407]]], grad_fn=<ViewBackward0>) torch.Size([2, 3, 2])\n",
            "tensor([[-0.5998,  0.4198],\n",
            "        [-0.7991,  0.8407]], grad_fn=<SliceBackward0>) torch.Size([2, 2])\n",
            "output post linear layer:  tensor([[0.5430],\n",
            "        [0.6204]], grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n",
            "output pre linear layer:  tensor([[[-0.8269,  0.8639],\n",
            "         [-0.8490,  0.8801],\n",
            "         [-0.8667,  0.8940]],\n",
            "\n",
            "        [[-0.8813,  0.9063],\n",
            "         [-0.8936,  0.9172],\n",
            "         [-0.9041,  0.9268]]], grad_fn=<ViewBackward0>) torch.Size([2, 3, 2])\n",
            "tensor([[-0.8667,  0.8940],\n",
            "        [-0.9041,  0.9268]], grad_fn=<SliceBackward0>) torch.Size([2, 2])\n",
            "output post linear layer:  tensor([[0.6257],\n",
            "        [0.6281]], grad_fn=<AddmmBackward0>) torch.Size([2, 1])\n"
          ]
        }
      ]
    }
  ]
}